# Troubleshooting Guide - VideoTeaching

H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c c√°c l·ªói th∆∞·ªùng g·∫∑p khi c√†i ƒë·∫∑t v√† ch·∫°y VideoTeaching.

## üì¶ Installation Issues

### 1. GLIBCXX Version Mismatch (Anaconda/Conda)

**L·ªói:**
```
OSError: /opt/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found
RuntimeError: Failed to load shared library 'libllama.so'
```

**Nguy√™n nh√¢n:** Anaconda's `libstdc++.so.6` c≈© h∆°n version m√† llama-cpp-python CUDA build y√™u c·∫ßu.

**Gi·∫£i ph√°p:**

#### Option 1: Update Anaconda's libstdc++ (Khuy·∫øn ngh·ªã)
```bash
# Activate conda base
conda activate base

# Update libstdc++
conda install -c conda-forge libstdcxx-ng

# Recreate venv
rm -rf .venv
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

#### Option 2: D√πng System Python thay v√¨ Anaconda
```bash
# Deactivate conda
conda deactivate

# D√πng system Python
/usr/bin/python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

#### Option 3: Install CPU-only llama-cpp-python
```bash
# Kh√¥ng d√πng CUDA - ch·ªâ CPU
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --force-reinstall --no-cache-dir \
    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
```

#### Option 4: Point to System libstdc++ (Temporary)
```bash
# Find system libstdc++
find /usr -name "libstdc++.so.6" 2>/dev/null

# Set LD_LIBRARY_PATH (add to ~/.bashrc for permanent)
export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH

# Restart Flask app
python run.py
```

### 2. torchvision.transforms.functional_tensor Not Found

**L·ªói:**
```
ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'
```

**Nguy√™n nh√¢n:** Torchvision version kh√¥ng t∆∞∆°ng th√≠ch v·ªõi basicsr/gfpgan. Module `functional_tensor` ƒë√£ ƒë∆∞·ª£c ƒë·ªïi t√™n trong torchvision m·ªõi.

**Gi·∫£i ph√°p:**

```bash
# Uninstall v√† reinstall torchvision v·ªõi ƒë√∫ng version
pip uninstall torchvision -y

# For CUDA 11.8 (khuy·∫øn ngh·ªã)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Ho·∫∑c ch·ªâ ƒë·ªãnh version c·ª• th·ªÉ
pip install 'torchvision>=0.15.0,<0.20.0'

# Reinstall basicsr v√† gfpgan
pip install basicsr>=1.4.2 gfpgan>=1.3.8 tb-nightly
```

**N·∫øu g·∫∑p l·ªói "Input/output error" khi install:**

```bash
# 1. Folder .venv b·ªã corrupt - T·∫°o venv m·ªõi ·ªü location kh√°c
deactivate
cd ~
python3 -m venv ~/VideoTeaching_venv
source ~/VideoTeaching_venv/bin/activate
cd /mnt/nvme1tb/trung/VideoTeaching

# Install packages
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt

# Ch·∫°y app v·ªõi venv m·ªõi
python run.py

# 2. Ho·∫∑c fix quy·ªÅn truy c·∫≠p folder hi·ªán t·∫°i
sudo chown -R $USER:$USER .venv
chmod -R u+w .venv

# 3. X√≥a v√† t·∫°o l·∫°i .venv t·∫°i ch·ªó
rm -rf .venv
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. llama-cpp-python Build Failed on Ubuntu/Linux

**L·ªói:**
```
/opt/anaconda3/compiler_compat/ld: warning: libgomp.so.1, needed by bin/libggml-cpu.so, not found
undefined reference to `GOMP_barrier@GOMP_1.0'
undefined reference to `GOMP_parallel@GOMP_4.0'
```

**Nguy√™n nh√¢n:** Thi·∫øu OpenMP library ho·∫∑c linker kh√¥ng t√¨m th·∫•y `libgomp`.

**Gi·∫£i ph√°p:**

#### Option 1: C√†i ƒë·∫∑t Pre-built Wheel (Khuy·∫øn ngh·ªã)

```bash
# C√†i ƒë·∫∑t pre-built wheel thay v√¨ build t·ª´ source
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
```

N·∫øu c√≥ GPU (CUDA):
```bash
# For CUDA 12.1
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# For CUDA 12.2
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122

# For CUDA 12.4
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124
```

#### Option 2: C√†i ƒë·∫∑t System Dependencies

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y \
    build-essential \
    cmake \
    gcc \
    g++ \
    libgomp1 \
    libopenblas-dev

# Sau ƒë√≥ c√†i l·∫°i
pip install llama-cpp-python>=0.3.16
```

#### Option 3: Set CMAKE Flags

```bash
# Set environment variables tr∆∞·ªõc khi install
export CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"
export FORCE_CMAKE=1

pip install llama-cpp-python>=0.3.16 --no-cache-dir
```

#### Option 4: Fix Linker Path (Anaconda/Conda)

N·∫øu d√πng Anaconda:
```bash
# Install gcc v√† libgomp t·ª´ conda-forge
conda install -c conda-forge gcc gxx libgomp

# Ho·∫∑c unset compiler_compat
unset LD_LIBRARY_PATH
pip install llama-cpp-python>=0.3.16
```

### 2. CUDA Out of Memory

**L·ªói:**
```
RuntimeError: CUDA out of memory
```

**Gi·∫£i ph√°p:**

```bash
# 1. S·ª≠ d·ª•ng model nh·ªè h∆°n
# Trong test_tts.html, ch·ªçn VieNeu-TTS-0.3B-q4-gguf thay v√¨ q8 ho·∫∑c full

# 2. Gi·∫£m batch size trong SadTalker
# Trong video_generator.py, gi·∫£m --batch_size t·ª´ 2 xu·ªëng 1

# 3. Clear CUDA cache
python -c "import torch; torch.cuda.empty_cache()"

# 4. Force CPU mode
export CUDA_VISIBLE_DEVICES=""
```

### 3. Transformers KeyError: 'tokenizers'

**L·ªói:**
```
KeyError: 'tokenizers'
File "/venv/lib/python3.10/site-packages/transformers/__init__.py"
```

**Gi·∫£i ph√°p:**

```bash
# Windows
Remove-Item -Recurse -Force venv\Lib\site-packages\transformers\__pycache__
Remove-Item -Recurse -Force venv\Lib\site-packages\tokenizers\__pycache__

# Linux/Mac
rm -rf venv/lib/python3.*/site-packages/transformers/__pycache__
rm -rf venv/lib/python3.*/site-packages/tokenizers/__pycache__

# Reinstall
pip install --upgrade --force-reinstall transformers tokenizers
```

### 4. Face Detection Failed - SadTalker

**L·ªói:**
```
ValueError: No face detected in the image
```

**Gi·∫£i ph√°p:**

1. **Ki·ªÉm tra ·∫£nh ƒë·∫ßu v√†o:**
   - S·ª≠ d·ª•ng ·∫£nh ch√¢n dung r√µ n√©t
   - Khu√¥n m·∫∑t ph·∫£i nh√¨n th·∫•y r√µ (kh√¥ng b·ªã che, blur)
   - √Ånh s√°ng t·ªët
   - Tr√°nh ·∫£nh g√≥c nghi√™ng qu√° nhi·ªÅu

2. **Code ƒë√£ c√≥ multi-threshold detection:**
   ```python
   # File: app/SadTalker/src/face3d/extract_kp_videos_safe.py
   # Th·ª≠ nhi·ªÅu confidence threshold: 0.97, 0.92, 0.85, 0.75
   ```

3. **N·∫øu v·∫´n l·ªói, th·ª≠ ·∫£nh kh√°c ho·∫∑c resize:**
   ```bash
   # Resize ·∫£nh v·ªÅ 512x512 tr∆∞·ªõc khi upload
   ```

### 5. FFmpeg Not Found

**L·ªói:**
```
FileNotFoundError: [Errno 2] No such file or directory: 'ffmpeg'
```

**Gi·∫£i ph√°p:**

```bash
# Ubuntu/Debian
sudo apt-get install -y ffmpeg

# macOS
brew install ffmpeg

# Windows
# Download t·ª´: https://ffmpeg.org/download.html
# Ho·∫∑c d√πng chocolatey:
choco install ffmpeg
```

## üê≥ Docker Issues

### 1. GPU Not Detected in Docker

**L·ªói:**
```
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]]
```

**Gi·∫£i ph√°p:**

```bash
# 1. Install NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# 2. Restart Docker
sudo systemctl restart docker

# 3. Test
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### 2. Docker Build Failed - Network Issues

**L·ªói:**
```
failed to fetch metadata: failed to resolve: no such host
```

**Gi·∫£i ph√°p:**

```bash
# 1. Set DNS in Docker
sudo nano /etc/docker/daemon.json

# Add:
{
  "dns": ["8.8.8.8", "8.8.4.4"]
}

# 2. Restart Docker
sudo systemctl restart docker

# 3. Build with --network=host
docker build --network=host -t videoteaching .
```

### 3. Permission Denied - Docker Volumes

**L·ªói:**
```
PermissionError: [Errno 13] Permission denied: '/app/static/results'
```

**Gi·∫£i ph√°p:**

```bash
# 1. Fix permissions on host
sudo chown -R $USER:$USER static/results static/uploads data

# 2. Ho·∫∑c th√™m v√†o Dockerfile:
RUN chmod -R 777 /app/static/results /app/static/uploads
```

## üîß Runtime Issues

### 1. Flask Port Already in Use

**L·ªói:**
```
OSError: [Errno 98] Address already in use
```

**Gi·∫£i ph√°p:**

```bash
# 1. Kill process tr√™n port 5000
# Linux/Mac:
lsof -ti:5000 | xargs kill -9

# Windows:
netstat -ano | findstr :5000
taskkill /PID <PID> /F

# 2. Ho·∫∑c ƒë·ªïi port trong config.py
# PORT = 8080
```

### 2. Model Download Slow/Failed

**L·ªói:**
```
HTTPError: 403 Client Error: Forbidden
ConnectionError: Failed to establish connection
```

**Gi·∫£i ph√°p:**

```bash
# 1. Set Hugging Face token (n·∫øu model private)
export HF_TOKEN=your_token_here

# 2. Set proxy (n·∫øu c√≥ firewall)
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# 3. TƒÉng timeout
export HF_HUB_DOWNLOAD_TIMEOUT=300

# 4. Download manual
huggingface-cli download pnnbao-ump/VieNeu-TTS-0.3B-q4-gguf
```

### 3. Gemini API Error

**L·ªói:**
```
google.api_core.exceptions.PermissionDenied: 403 API key not valid
```

**Gi·∫£i ph√°p:**

```bash
# 1. Ki·ªÉm tra API key
echo $GEMINI_API_KEY

# 2. T·∫°o API key m·ªõi t·∫°i:
# https://makersuite.google.com/app/apikey

# 3. Update .env
GEMINI_API_KEY=your_new_api_key_here

# 4. Restart app
```

## üìä Performance Issues

### 1. TTS Generation Too Slow

**Gi·∫£i ph√°p:**

```bash
# 1. S·ª≠ d·ª•ng GGUF Q4 model (fastest)
# Trong UI ch·ªçn: VieNeu-TTS-0.3B-q4-gguf

# 2. Set threads
export OMP_NUM_THREADS=8

# 3. N·∫øu c√≥ GPU, d√πng PyTorch model
# VieNeu-TTS-0.3B ho·∫∑c VieNeu-TTS
```

### 2. Video Generation Takes Too Long

**Gi·∫£i ph√°p:**

```python
# Trong video_generator.py, ƒëi·ªÅu ch·ªânh parameters:

'--size', '256',           # Gi·∫£m t·ª´ 512 ‚Üí 256
'--batch_size', '1',       # Gi·∫£m batch size
'--preprocess', 'crop',    # Thay v√¨ 'full'
# B·ªè '--enhancer', 'gfpgan'  # Skip face enhancement
```

## üîç Debugging Tips

### Check Library Versions

```bash
# Check GLIBCXX versions available
strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX

# For Anaconda
strings /opt/anaconda3/lib/libstdc++.so.6 | grep GLIBCXX

# Check llama-cpp-python build info
python -c "import llama_cpp; print(llama_cpp.__version__)"
```

### Enable Debug Mode

```bash
# .env
FLASK_ENV=development
FLASK_DEBUG=1
```

### Check Logs

```bash
# Flask app logs
tail -f logs/app.log

# Docker logs
docker-compose logs -f videoteaching

# System logs
journalctl -u docker -f
```

### Test Components Individually

```bash
# Test TTS
curl -X POST http://localhost:5000/api/generate-tts \
  -F "text=Xin ch√†o" \
  -F "voice=default" \
  -F "model=pnnbao-ump/VieNeu-TTS-0.3B-q4-gguf"

# Test SadTalker
curl -X POST http://localhost:5000/api/generate-video \
  -F "image=@test.jpg" \
  -F "audio=@test.wav"
```

## üÜò Still Having Issues?

1. **Check System Requirements:**
   - Python 3.10+
   - CUDA 11.8+ (for GPU)
   - 8GB+ RAM (16GB recommended)
   - 10GB+ disk space

2. **Update Dependencies:**
   ```bash
   pip install --upgrade -r requirements.txt
   ```

3. **Clean Install:**
   ```bash
   # Backup data first!
   rm -rf venv
   python -m venv venv
   source venv/bin/activate  # or venv\Scripts\activate on Windows
   pip install -r requirements.txt
   ```

4. **Report Issue:**
   - GitHub Issues: https://github.com/Cong-ty-TNNH-MoneyEveryWhere/CreateVideo-Website/issues
   - Include: OS, Python version, error logs, steps to reproduce

---

**Last updated:** January 28, 2026
